# Logstash pipeline configuration for Prismatic Collections logs

input {
  beats {
    port => 5044
  }

  tcp {
    port => 5000
    codec => json_lines
  }

  udp {
    port => 5000
    codec => json_lines
  }
}

filter {
  # Parse Docker container logs
  if [container] {
    mutate {
      add_field => { "service" => "%{[container][name]}" }
    }

    # Remove container prefix from service name
    mutate {
      gsub => [ "service", "^prismatic-", "" ]
    }
  }

  # Parse JSON logs
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
      target => "parsed"
    }
  }

  # Parse timestamp
  if [parsed][timestamp] {
    date {
      match => [ "[parsed][timestamp]", "ISO8601" ]
    }
  }

  # Add log level field
  if [parsed][level] {
    mutate {
      add_field => { "log_level" => "%{[parsed][level]}" }
    }
  }

  # Parse backend API logs
  if [service] == "backend" {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{WORD:level}\] %{DATA:logger}: %{GREEDYDATA:msg}"
      }
    }
  }

  # Parse nginx access logs
  if [service] == "frontend" and [message] =~ /^\d+\.\d+\.\d+\.\d+/ {
    grok {
      match => {
        "message" => "%{COMBINEDAPACHELOG}"
      }
    }

    mutate {
      convert => { "response" => "integer" }
      convert => { "bytes" => "integer" }
    }
  }

  # Parse Python fetcher logs
  if [service] == "twitter-fetcher" {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} - %{WORD:level} - %{GREEDYDATA:msg}"
      }
    }
  }

  # Remove unnecessary fields
  mutate {
    remove_field => [ "agent", "ecs", "input", "log" ]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "prismatic-%{service}-%{+YYYY.MM.dd}"
  }

  # Debug output (remove in production)
  # stdout {
  #   codec => rubydebug
  # }
}